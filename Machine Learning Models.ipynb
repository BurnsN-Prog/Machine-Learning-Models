{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "This project aims to develop classification models to analyze subscribers' behavior and predict user plans. The models will be used to recommend subscribers of Megaline's legacy plans, one of their newer plans, Smart, or Ultra. \n",
    "\n",
    "Using the `users_behavior.csv` dataset, I tested various models with different hyperparameters. The models' threshold of success is 0.75 or 75%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('users_behavior.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['calls', 'minutes', 'messages', 'mb_used', 'is_ultra'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3214 entries, 0 to 3213\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   calls     3214 non-null   float64\n",
      " 1   minutes   3214 non-null   float64\n",
      " 2   messages  3214 non-null   float64\n",
      " 3   mb_used   3214 non-null   float64\n",
      " 4   is_ultra  3214 non-null   int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 125.7 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>calls</th>\n",
       "      <th>minutes</th>\n",
       "      <th>messages</th>\n",
       "      <th>mb_used</th>\n",
       "      <th>is_ultra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.0</td>\n",
       "      <td>311.90</td>\n",
       "      <td>83.0</td>\n",
       "      <td>19915.42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85.0</td>\n",
       "      <td>516.75</td>\n",
       "      <td>56.0</td>\n",
       "      <td>22696.96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77.0</td>\n",
       "      <td>467.66</td>\n",
       "      <td>86.0</td>\n",
       "      <td>21060.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>106.0</td>\n",
       "      <td>745.53</td>\n",
       "      <td>81.0</td>\n",
       "      <td>8437.39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66.0</td>\n",
       "      <td>418.74</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14502.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   calls  minutes  messages   mb_used  is_ultra\n",
       "0   40.0   311.90      83.0  19915.42         0\n",
       "1   85.0   516.75      56.0  22696.96         0\n",
       "2   77.0   467.66      86.0  21060.45         0\n",
       "3  106.0   745.53      81.0   8437.39         1\n",
       "4   66.0   418.74       1.0  14502.75         0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into features and target, then into train, validation, and test set(3:1:1 split).\n",
    "features = df.drop('is_ultra', axis= 1)\n",
    "target= df['is_ultra']\n",
    "features_train,temp_train, target_train,temp_target= train_test_split(features, target, test_size= 0.40, random_state=42) \n",
    "features_validation, features_test, target_validation, target_test= train_test_split(temp_train, temp_target, test_size=0.50, random_state=42) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation \n",
    "\n",
    "The lesson mentioned a `3:1:1` (60, 20, 20) split being the best, so this was the logic used in my split. I started with a 60/40 split for a training and temporary set, then followed up by splitting the temporary set in half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Depth: 3 Accuracy Score: 0.7916018662519441\n"
     ]
    }
   ],
   "source": [
    "# Finding the best hyperparameters for a Decision Tree model \n",
    "best_decision_model= None\n",
    "best_depth= 0\n",
    "best_results= 0\n",
    "\n",
    "for depth in range(1,6):\n",
    "    model=DecisionTreeClassifier(random_state=42, max_depth=depth)\n",
    "    model.fit(features_train, target_train)\n",
    "    predictions= model.predict(features_validation)\n",
    "    results = accuracy_score(target_validation, predictions)\n",
    "    if results > best_results:\n",
    "        best_decision_model= model\n",
    "        best_depth= depth\n",
    "        best_results= results\n",
    "print(f'Best Depth: {best_depth} Accuracy Score: {best_results}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test set's accuracy score is: 0.8055987558320373\n"
     ]
    }
   ],
   "source": [
    "# Assessing the quality of the best decision tree model on the test set\n",
    "test_predictions= best_decision_model.predict(features_test)\n",
    "test_accuracy_score= accuracy_score(target_test,test_predictions)\n",
    "print(\"The test set's accuracy score is:\", test_accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "The Decision Tree model with a depth of 3 achieved a validation accuracy of `79.2%` and a test accuracy of `80.6%`, both exceeding the assignment's required threshold of 75%. The higher test accuracy compared to the validation set suggests that the model generalizes well to unseen data without signs of overfitting or underfitting. Overall, the decision tree demonstrates strong performance on this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best number of estimators: 4 Best Score: 0.7916018662519441\n"
     ]
    }
   ],
   "source": [
    "# Finding the best estimators for a Random Forest model\n",
    "best_rf_model= None\n",
    "best_est= 0\n",
    "best_score= 0\n",
    "\n",
    "for est in range(1, 8):\n",
    "    model= RandomForestClassifier(n_estimators=est, random_state=42)\n",
    "    model.fit(features_train, target_train)\n",
    "    score= model.score(features_validation, target_validation)\n",
    "    if score > best_score:\n",
    "        best_rf_model = model\n",
    "        best_est = est\n",
    "        best_score= score\n",
    "print(f'Best number of estimators: {best_est} Best Score: {best_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test set's score is: 0.8009331259720062\n"
     ]
    }
   ],
   "source": [
    "# Assessing the quality of the best random forest model on the test set\n",
    "test_score = best_rf_model.score(features_test, target_test)\n",
    "print(f\"The test set's score is:\", test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "The Random Forest model with an estimators count of 4 achieved a validation accuracy of `79.2%` and a test accuracy of `80.1%`, performing nearly identical to the decision tree model while maintaining a consistent performance on unseen data. The validation and test scores suggest the model generalizes well without signs of overfitting or underfitting. Since the model surpasses the assignment’s target threshold of 75%, it can be considered successful and reliable for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Score: 0.7076205287713841\n"
     ]
    }
   ],
   "source": [
    "# Using a Logistic Regression model \n",
    "lr_model = LogisticRegression(solver='liblinear', random_state=42)\n",
    "lr_model.fit(features_train, target_train)\n",
    "lr_score = lr_model.score(features_validation, target_validation)\n",
    "print('Logistic Regression Score:', lr_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR model score on the test set: 0.7076205287713841\n"
     ]
    }
   ],
   "source": [
    "# Using the LR model on the test set\n",
    "test_score= lr_model.score(features_test, target_test)\n",
    "print(f'LR model score on the test set:', test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "The logistic regression model produced identical accuracy scores of `70.76%` on both the validation and test sets, indicating strong consistency and generalization with no signs of overfitting or underfitting. However, this model missed the assignment’s required accuracy threshold of 75%, making it the weakest performer among the models tested. This result suggests that logistic regression may not be complex enough to capture the underlying patterns in this particular dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Thoughts\n",
    "\n",
    "Based on the parameters of the assignment, I was tasked with developing several classification models, using accuracy as the sole evaluation metric. Given this constraint, both the decision tree and random forest models performed well, with very similar results. However, if I had to choose one, I would choose to deploy the decision tree, as it achieved slightly better accuracy on unseen test data.\n",
    "\n",
    "Going forward, I would consider testing additional hyperparameters and incorporating other evaluation metrics, such as precision, recall, or F1-score, to gain more understanding of the models' performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
